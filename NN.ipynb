{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWEKrLmwvYWq6nt3mE0HPl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S-Ali-S/NLP/blob/main/NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Sigmoid and Softmax activation functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # To avoid overflow\n",
        "    return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Filter out only digits 1, 2, and 3\n",
        "filter_train = np.isin(y_train, [1, 2, 3])\n",
        "filter_test = np.isin(y_test, [1, 2, 3])\n",
        "\n",
        "x_train_filtered = x_train[filter_train]\n",
        "y_train_filtered = y_train[filter_train]\n",
        "x_test_filtered = x_test[filter_test]\n",
        "y_test_filtered = y_test[filter_test]\n",
        "\n",
        "# Select 20 samples for each of the digits 1, 2, and 3\n",
        "x_train_final = []\n",
        "y_train_final = []\n",
        "x_test_final = []\n",
        "y_test_final = []\n",
        "\n",
        "for digit in [1, 2, 3]:\n",
        "    x_train_digit = x_train_filtered[y_train_filtered == digit][:20]\n",
        "    y_train_digit = y_train_filtered[y_train_filtered == digit][:20]\n",
        "    x_train_final.append(x_train_digit)\n",
        "    y_train_final.append(y_train_digit)\n",
        "\n",
        "    x_test_digit = x_test_filtered[y_test_filtered == digit][:20]\n",
        "    y_test_digit = y_test_filtered[y_test_filtered == digit][:20]\n",
        "    x_test_final.append(x_test_digit)\n",
        "    y_test_final.append(y_test_digit)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "x_train_final = np.vstack(x_train_final)\n",
        "y_train_final = np.hstack(y_train_final)\n",
        "x_test_final = np.vstack(x_test_final)\n",
        "y_test_final = np.hstack(y_test_final)\n",
        "\n",
        "# Normalize the data\n",
        "x_train_final = x_train_final.astype('float32') / 255.0\n",
        "x_test_final = x_test_final.astype('float32') / 255.0\n",
        "\n",
        "# Subtract 1 from the labels to shift 1, 2, 3 -> 0, 1, 2\n",
        "y_train_final -= 1\n",
        "y_test_final -= 1\n",
        "\n",
        "# One-hot encode the labels (now they are 0, 1, 2)\n",
        "y_train_final = np.array([to_categorical(y, 3) for y in y_train_final])\n",
        "y_test_final = np.array([to_categorical(y, 3) for y in y_test_final])\n",
        "\n",
        "# Reshape data to flatten the images (28x28 -> 784)\n",
        "x_train_final = x_train_final.reshape(-1, 28 * 28)\n",
        "x_test_final = x_test_final.reshape(-1, 28 * 28)\n",
        "\n",
        "# Initialize weights and biases\n",
        "input_size = 28 * 28  # 784 features (28x28 pixels)\n",
        "hidden_size = 10      # 10 neurons in the hidden layer\n",
        "output_size = 3       # 3 possible outputs (for digits 1, 2, and 3)\n",
        "\n",
        "# Random initialization of weights and biases\n",
        "np.random.seed(42)\n",
        "weights_input_hidden = np.random.randn(input_size, hidden_size) * 0.01\n",
        "bias_hidden = np.zeros((1, hidden_size))\n",
        "weights_hidden_output = np.random.randn(hidden_size, output_size) * 0.01\n",
        "bias_output = np.zeros((1, output_size))\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 2000\n",
        "\n",
        "# Training the neural network\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    hidden_input = np.dot(x_train_final, weights_input_hidden) + bias_hidden\n",
        "    hidden_output = sigmoid(hidden_input)\n",
        "\n",
        "    output_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
        "    output = softmax(output_input)\n",
        "\n",
        "    # Compute the loss (cross-entropy)\n",
        "    loss = -np.sum(y_train_final * np.log(output + 1e-9)) / x_train_final.shape[0]\n",
        "\n",
        "    # Backpropagation\n",
        "    output_error = output - y_train_final\n",
        "    hidden_error = np.dot(output_error, weights_hidden_output.T) * sigmoid_derivative(hidden_output)\n",
        "\n",
        "    # Gradients for weights and biases\n",
        "    grad_weights_hidden_output = np.dot(hidden_output.T, output_error) / x_train_final.shape[0]\n",
        "    grad_bias_output = np.sum(output_error, axis=0, keepdims=True) / x_train_final.shape[0]\n",
        "    grad_weights_input_hidden = np.dot(x_train_final.T, hidden_error) / x_train_final.shape[0]\n",
        "    grad_bias_hidden = np.sum(hidden_error, axis=0, keepdims=True) / x_train_final.shape[0]\n",
        "\n",
        "    # Update weights and biases\n",
        "    weights_input_hidden -= learning_rate * grad_weights_input_hidden\n",
        "    bias_hidden -= learning_rate * grad_bias_hidden\n",
        "    weights_hidden_output -= learning_rate * grad_weights_hidden_output\n",
        "    bias_output -= learning_rate * grad_bias_output\n",
        "\n",
        "    # Print loss every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}')\n",
        "\n",
        "# Evaluate on the test set\n",
        "hidden_input = np.dot(x_test_final, weights_input_hidden) + bias_hidden\n",
        "hidden_output = sigmoid(hidden_input)\n",
        "\n",
        "output_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
        "output = softmax(output_input)\n",
        "\n",
        "# Accuracy calculation\n",
        "predictions = np.argmax(output, axis=1)\n",
        "\n",
        "# Print predicted and desired labels for each test sample\n",
        "for i in range(len(predictions)):\n",
        "    predicted_label = predictions[i] + 1  # Convert 0,1,2 back to 1,2,3\n",
        "    desired_label = np.argmax(y_test_final[i]) + 1  # Convert 0,1,2 back to 1,2,3\n",
        "    print(f\"Predicted: {predicted_label}, Desired: {desired_label}\")\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = np.mean(predictions == np.argmax(y_test_final, axis=1))\n",
        "print(f'Test accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZluPInSsVoD",
        "outputId": "5bbc2f16-5e44-43fd-cd51-a930269197fa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000, Loss: 1.0988\n",
            "Epoch 11/2000, Loss: 1.0987\n",
            "Epoch 21/2000, Loss: 1.0986\n",
            "Epoch 31/2000, Loss: 1.0986\n",
            "Epoch 41/2000, Loss: 1.0985\n",
            "Epoch 51/2000, Loss: 1.0985\n",
            "Epoch 61/2000, Loss: 1.0985\n",
            "Epoch 71/2000, Loss: 1.0984\n",
            "Epoch 81/2000, Loss: 1.0984\n",
            "Epoch 91/2000, Loss: 1.0983\n",
            "Epoch 101/2000, Loss: 1.0983\n",
            "Epoch 111/2000, Loss: 1.0982\n",
            "Epoch 121/2000, Loss: 1.0982\n",
            "Epoch 131/2000, Loss: 1.0981\n",
            "Epoch 141/2000, Loss: 1.0981\n",
            "Epoch 151/2000, Loss: 1.0980\n",
            "Epoch 161/2000, Loss: 1.0979\n",
            "Epoch 171/2000, Loss: 1.0979\n",
            "Epoch 181/2000, Loss: 1.0978\n",
            "Epoch 191/2000, Loss: 1.0978\n",
            "Epoch 201/2000, Loss: 1.0977\n",
            "Epoch 211/2000, Loss: 1.0976\n",
            "Epoch 221/2000, Loss: 1.0975\n",
            "Epoch 231/2000, Loss: 1.0974\n",
            "Epoch 241/2000, Loss: 1.0974\n",
            "Epoch 251/2000, Loss: 1.0973\n",
            "Epoch 261/2000, Loss: 1.0972\n",
            "Epoch 271/2000, Loss: 1.0971\n",
            "Epoch 281/2000, Loss: 1.0970\n",
            "Epoch 291/2000, Loss: 1.0968\n",
            "Epoch 301/2000, Loss: 1.0967\n",
            "Epoch 311/2000, Loss: 1.0966\n",
            "Epoch 321/2000, Loss: 1.0965\n",
            "Epoch 331/2000, Loss: 1.0963\n",
            "Epoch 341/2000, Loss: 1.0962\n",
            "Epoch 351/2000, Loss: 1.0960\n",
            "Epoch 361/2000, Loss: 1.0958\n",
            "Epoch 371/2000, Loss: 1.0956\n",
            "Epoch 381/2000, Loss: 1.0954\n",
            "Epoch 391/2000, Loss: 1.0952\n",
            "Epoch 401/2000, Loss: 1.0950\n",
            "Epoch 411/2000, Loss: 1.0948\n",
            "Epoch 421/2000, Loss: 1.0945\n",
            "Epoch 431/2000, Loss: 1.0942\n",
            "Epoch 441/2000, Loss: 1.0939\n",
            "Epoch 451/2000, Loss: 1.0936\n",
            "Epoch 461/2000, Loss: 1.0933\n",
            "Epoch 471/2000, Loss: 1.0930\n",
            "Epoch 481/2000, Loss: 1.0926\n",
            "Epoch 491/2000, Loss: 1.0922\n",
            "Epoch 501/2000, Loss: 1.0918\n",
            "Epoch 511/2000, Loss: 1.0914\n",
            "Epoch 521/2000, Loss: 1.0909\n",
            "Epoch 531/2000, Loss: 1.0904\n",
            "Epoch 541/2000, Loss: 1.0899\n",
            "Epoch 551/2000, Loss: 1.0893\n",
            "Epoch 561/2000, Loss: 1.0887\n",
            "Epoch 571/2000, Loss: 1.0881\n",
            "Epoch 581/2000, Loss: 1.0875\n",
            "Epoch 591/2000, Loss: 1.0868\n",
            "Epoch 601/2000, Loss: 1.0860\n",
            "Epoch 611/2000, Loss: 1.0852\n",
            "Epoch 621/2000, Loss: 1.0844\n",
            "Epoch 631/2000, Loss: 1.0835\n",
            "Epoch 641/2000, Loss: 1.0826\n",
            "Epoch 651/2000, Loss: 1.0816\n",
            "Epoch 661/2000, Loss: 1.0806\n",
            "Epoch 671/2000, Loss: 1.0795\n",
            "Epoch 681/2000, Loss: 1.0783\n",
            "Epoch 691/2000, Loss: 1.0771\n",
            "Epoch 701/2000, Loss: 1.0758\n",
            "Epoch 711/2000, Loss: 1.0744\n",
            "Epoch 721/2000, Loss: 1.0730\n",
            "Epoch 731/2000, Loss: 1.0715\n",
            "Epoch 741/2000, Loss: 1.0699\n",
            "Epoch 751/2000, Loss: 1.0682\n",
            "Epoch 761/2000, Loss: 1.0664\n",
            "Epoch 771/2000, Loss: 1.0646\n",
            "Epoch 781/2000, Loss: 1.0626\n",
            "Epoch 791/2000, Loss: 1.0606\n",
            "Epoch 801/2000, Loss: 1.0584\n",
            "Epoch 811/2000, Loss: 1.0562\n",
            "Epoch 821/2000, Loss: 1.0538\n",
            "Epoch 831/2000, Loss: 1.0513\n",
            "Epoch 841/2000, Loss: 1.0487\n",
            "Epoch 851/2000, Loss: 1.0460\n",
            "Epoch 861/2000, Loss: 1.0432\n",
            "Epoch 871/2000, Loss: 1.0402\n",
            "Epoch 881/2000, Loss: 1.0371\n",
            "Epoch 891/2000, Loss: 1.0339\n",
            "Epoch 901/2000, Loss: 1.0305\n",
            "Epoch 911/2000, Loss: 1.0270\n",
            "Epoch 921/2000, Loss: 1.0233\n",
            "Epoch 931/2000, Loss: 1.0195\n",
            "Epoch 941/2000, Loss: 1.0155\n",
            "Epoch 951/2000, Loss: 1.0114\n",
            "Epoch 961/2000, Loss: 1.0071\n",
            "Epoch 971/2000, Loss: 1.0027\n",
            "Epoch 981/2000, Loss: 0.9980\n",
            "Epoch 991/2000, Loss: 0.9933\n",
            "Epoch 1001/2000, Loss: 0.9883\n",
            "Epoch 1011/2000, Loss: 0.9832\n",
            "Epoch 1021/2000, Loss: 0.9779\n",
            "Epoch 1031/2000, Loss: 0.9724\n",
            "Epoch 1041/2000, Loss: 0.9667\n",
            "Epoch 1051/2000, Loss: 0.9609\n",
            "Epoch 1061/2000, Loss: 0.9549\n",
            "Epoch 1071/2000, Loss: 0.9487\n",
            "Epoch 1081/2000, Loss: 0.9423\n",
            "Epoch 1091/2000, Loss: 0.9358\n",
            "Epoch 1101/2000, Loss: 0.9290\n",
            "Epoch 1111/2000, Loss: 0.9222\n",
            "Epoch 1121/2000, Loss: 0.9151\n",
            "Epoch 1131/2000, Loss: 0.9079\n",
            "Epoch 1141/2000, Loss: 0.9005\n",
            "Epoch 1151/2000, Loss: 0.8929\n",
            "Epoch 1161/2000, Loss: 0.8852\n",
            "Epoch 1171/2000, Loss: 0.8773\n",
            "Epoch 1181/2000, Loss: 0.8693\n",
            "Epoch 1191/2000, Loss: 0.8612\n",
            "Epoch 1201/2000, Loss: 0.8529\n",
            "Epoch 1211/2000, Loss: 0.8445\n",
            "Epoch 1221/2000, Loss: 0.8360\n",
            "Epoch 1231/2000, Loss: 0.8273\n",
            "Epoch 1241/2000, Loss: 0.8186\n",
            "Epoch 1251/2000, Loss: 0.8097\n",
            "Epoch 1261/2000, Loss: 0.8008\n",
            "Epoch 1271/2000, Loss: 0.7918\n",
            "Epoch 1281/2000, Loss: 0.7827\n",
            "Epoch 1291/2000, Loss: 0.7736\n",
            "Epoch 1301/2000, Loss: 0.7644\n",
            "Epoch 1311/2000, Loss: 0.7551\n",
            "Epoch 1321/2000, Loss: 0.7459\n",
            "Epoch 1331/2000, Loss: 0.7365\n",
            "Epoch 1341/2000, Loss: 0.7272\n",
            "Epoch 1351/2000, Loss: 0.7179\n",
            "Epoch 1361/2000, Loss: 0.7085\n",
            "Epoch 1371/2000, Loss: 0.6992\n",
            "Epoch 1381/2000, Loss: 0.6899\n",
            "Epoch 1391/2000, Loss: 0.6806\n",
            "Epoch 1401/2000, Loss: 0.6713\n",
            "Epoch 1411/2000, Loss: 0.6620\n",
            "Epoch 1421/2000, Loss: 0.6528\n",
            "Epoch 1431/2000, Loss: 0.6437\n",
            "Epoch 1441/2000, Loss: 0.6346\n",
            "Epoch 1451/2000, Loss: 0.6255\n",
            "Epoch 1461/2000, Loss: 0.6166\n",
            "Epoch 1471/2000, Loss: 0.6077\n",
            "Epoch 1481/2000, Loss: 0.5989\n",
            "Epoch 1491/2000, Loss: 0.5901\n",
            "Epoch 1501/2000, Loss: 0.5815\n",
            "Epoch 1511/2000, Loss: 0.5729\n",
            "Epoch 1521/2000, Loss: 0.5644\n",
            "Epoch 1531/2000, Loss: 0.5561\n",
            "Epoch 1541/2000, Loss: 0.5478\n",
            "Epoch 1551/2000, Loss: 0.5396\n",
            "Epoch 1561/2000, Loss: 0.5315\n",
            "Epoch 1571/2000, Loss: 0.5236\n",
            "Epoch 1581/2000, Loss: 0.5157\n",
            "Epoch 1591/2000, Loss: 0.5080\n",
            "Epoch 1601/2000, Loss: 0.5003\n",
            "Epoch 1611/2000, Loss: 0.4928\n",
            "Epoch 1621/2000, Loss: 0.4854\n",
            "Epoch 1631/2000, Loss: 0.4781\n",
            "Epoch 1641/2000, Loss: 0.4709\n",
            "Epoch 1651/2000, Loss: 0.4639\n",
            "Epoch 1661/2000, Loss: 0.4569\n",
            "Epoch 1671/2000, Loss: 0.4501\n",
            "Epoch 1681/2000, Loss: 0.4433\n",
            "Epoch 1691/2000, Loss: 0.4367\n",
            "Epoch 1701/2000, Loss: 0.4302\n",
            "Epoch 1711/2000, Loss: 0.4238\n",
            "Epoch 1721/2000, Loss: 0.4176\n",
            "Epoch 1731/2000, Loss: 0.4114\n",
            "Epoch 1741/2000, Loss: 0.4054\n",
            "Epoch 1751/2000, Loss: 0.3994\n",
            "Epoch 1761/2000, Loss: 0.3936\n",
            "Epoch 1771/2000, Loss: 0.3878\n",
            "Epoch 1781/2000, Loss: 0.3822\n",
            "Epoch 1791/2000, Loss: 0.3767\n",
            "Epoch 1801/2000, Loss: 0.3713\n",
            "Epoch 1811/2000, Loss: 0.3659\n",
            "Epoch 1821/2000, Loss: 0.3607\n",
            "Epoch 1831/2000, Loss: 0.3556\n",
            "Epoch 1841/2000, Loss: 0.3506\n",
            "Epoch 1851/2000, Loss: 0.3456\n",
            "Epoch 1861/2000, Loss: 0.3408\n",
            "Epoch 1871/2000, Loss: 0.3360\n",
            "Epoch 1881/2000, Loss: 0.3314\n",
            "Epoch 1891/2000, Loss: 0.3268\n",
            "Epoch 1901/2000, Loss: 0.3223\n",
            "Epoch 1911/2000, Loss: 0.3179\n",
            "Epoch 1921/2000, Loss: 0.3136\n",
            "Epoch 1931/2000, Loss: 0.3093\n",
            "Epoch 1941/2000, Loss: 0.3052\n",
            "Epoch 1951/2000, Loss: 0.3011\n",
            "Epoch 1961/2000, Loss: 0.2971\n",
            "Epoch 1971/2000, Loss: 0.2932\n",
            "Epoch 1981/2000, Loss: 0.2893\n",
            "Epoch 1991/2000, Loss: 0.2855\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 3, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 1, Desired: 1\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 2, Desired: 2\n",
            "Predicted: 3, Desired: 2\n",
            "Predicted: 2, Desired: 3\n",
            "Predicted: 3, Desired: 3\n",
            "Predicted: 3, Desired: 3\n",
            "Predicted: 3, Desired: 3\n",
            "Predicted: 3, Desired: 3\n",
            "Predicted: 2, Desired: 3\n",
            "Predicted: 3, Desired: 3\n",
            "Predicted: 3, Desired: 3\n",
            "Predicted: 3, Desired: 3\n",
            "Predicted: 3, Desired: 3\n",
            "Predicted: 3, Desired: 3\n",
            "Predicted: 3, Desired: 3\n",
            "Predicted: 3, Desired: 3\n",
            "Predicted: 2, Desired: 3\n",
            "Predicted: 3, Desired: 3\n",
            "Predicted: 3, Desired: 3\n",
            "Predicted: 2, Desired: 3\n",
            "Predicted: 3, Desired: 3\n",
            "Predicted: 1, Desired: 3\n",
            "Predicted: 3, Desired: 3\n",
            "Test accuracy: 88.33%\n"
          ]
        }
      ]
    }
  ]
}