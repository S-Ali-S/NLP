{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQ6S2Q+UZHVtKvJhvz7Bky"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0NbAWhA0oel4","executionInfo":{"status":"ok","timestamp":1672918035656,"user_tz":-210,"elapsed":400686,"user":{"displayName":"Sayed Ali Sharifian","userId":"17165198639354888633"}},"outputId":"a15d8382-9804-45ee-da5c-fc0c5ac0d71d"},"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 3, 5, 7, 1, 5, 6]\n","i write rewrite and [UNK] rewrite again\n","['', '[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n","tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n","i write rewrite and [UNK] rewrite again\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 80.2M  100 80.2M    0     0  36.0M      0  0:00:02  0:00:02 --:--:-- 36.0M\n","I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />EnjoyFound 20000 files belonging to 2 classes.\n","Found 5000 files belonging to 2 classes.\n","Found 25000 files belonging to 2 classes.\n","inputs.shape: (32,)\n","inputs.dtype: <dtype: 'string'>\n","targets.shape: (32,)\n","targets.dtype: <dtype: 'int32'>\n","inputs[0]: tf.Tensor(b\"This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've already done for the film. I can't go on enough about this horrible movie, its almost something that Ed Wood would have made and in that case it surely would have been his masterpiece.<br /><br />To start you are forced to sit through an opening dialogue the likes of which you've never seen/heard, this thing has got to be five minutes long. On top of that it is narrated, as to suggest that you the viewer cannot read. Then we meet Mr. Slater and the barrage of terrible lines gets underway, it is as if he is operating solely to get lines on to the movie poster tag line. Soon we meet Stephen Dorff, who I typically enjoy) and he does his best not to drown in this but ultimately he does. Then comes the ultimate insult, Tara Reid playing an intelligent role, oh help us! Tara Reid is not a very talented actress and somehow she continually gets roles in movies, in my opinion though she should stick to movies of the American pie type. <br /><br />All in all you just may want to see this for yourself when it comes out on video, I know that I got a kick out of it, I mean lets all be honest here, sometimes its comforting to revel in the shortcomings of others.\", shape=(), dtype=string)\n","targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n","inputs.shape: (32, 20000)\n","inputs.dtype: <dtype: 'float32'>\n","targets.shape: (32,)\n","targets.dtype: <dtype: 'int32'>\n","inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n","targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 20000)]           0         \n","                                                                 \n"," dense (Dense)               (None, 16)                320016    \n","                                                                 \n"," dropout (Dropout)           (None, 16)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 17        \n","                                                                 \n","=================================================================\n","Total params: 320,033\n","Trainable params: 320,033\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","625/625 [==============================] - 15s 22ms/step - loss: 0.4138 - accuracy: 0.8278 - val_loss: 0.2768 - val_accuracy: 0.8956\n","Epoch 2/10\n","625/625 [==============================] - 6s 10ms/step - loss: 0.2719 - accuracy: 0.9016 - val_loss: 0.2644 - val_accuracy: 0.8968\n","Epoch 3/10\n","625/625 [==============================] - 7s 11ms/step - loss: 0.2395 - accuracy: 0.9169 - val_loss: 0.2834 - val_accuracy: 0.8970\n","Epoch 4/10\n","625/625 [==============================] - 6s 10ms/step - loss: 0.2273 - accuracy: 0.9230 - val_loss: 0.2928 - val_accuracy: 0.8966\n","Epoch 5/10\n","625/625 [==============================] - 6s 9ms/step - loss: 0.2226 - accuracy: 0.9280 - val_loss: 0.3115 - val_accuracy: 0.8942\n","Epoch 6/10\n","625/625 [==============================] - 6s 10ms/step - loss: 0.2129 - accuracy: 0.9317 - val_loss: 0.3151 - val_accuracy: 0.8964\n","Epoch 7/10\n","625/625 [==============================] - 6s 9ms/step - loss: 0.2115 - accuracy: 0.9344 - val_loss: 0.3266 - val_accuracy: 0.8960\n","Epoch 8/10\n","625/625 [==============================] - 7s 10ms/step - loss: 0.2076 - accuracy: 0.9338 - val_loss: 0.3311 - val_accuracy: 0.8944\n","Epoch 9/10\n","625/625 [==============================] - 6s 10ms/step - loss: 0.2101 - accuracy: 0.9347 - val_loss: 0.3501 - val_accuracy: 0.8936\n","Epoch 10/10\n","625/625 [==============================] - 6s 9ms/step - loss: 0.2018 - accuracy: 0.9374 - val_loss: 0.3500 - val_accuracy: 0.8918\n","782/782 [==============================] - 10s 12ms/step - loss: 0.2907 - accuracy: 0.8889\n","Test acc: 0.889\n","Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 20000)]           0         \n","                                                                 \n"," dense_2 (Dense)             (None, 16)                320016    \n","                                                                 \n"," dropout_1 (Dropout)         (None, 16)                0         \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 17        \n","                                                                 \n","=================================================================\n","Total params: 320,033\n","Trainable params: 320,033\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","625/625 [==============================] - 14s 22ms/step - loss: 0.3896 - accuracy: 0.8374 - val_loss: 0.2644 - val_accuracy: 0.8984\n","Epoch 2/10\n","625/625 [==============================] - 7s 11ms/step - loss: 0.2480 - accuracy: 0.9119 - val_loss: 0.2962 - val_accuracy: 0.8920\n","Epoch 3/10\n","625/625 [==============================] - 6s 9ms/step - loss: 0.2194 - accuracy: 0.9283 - val_loss: 0.3082 - val_accuracy: 0.8960\n","Epoch 4/10\n","625/625 [==============================] - 6s 10ms/step - loss: 0.2039 - accuracy: 0.9366 - val_loss: 0.3161 - val_accuracy: 0.8986\n","Epoch 5/10\n","625/625 [==============================] - 6s 9ms/step - loss: 0.1860 - accuracy: 0.9405 - val_loss: 0.3150 - val_accuracy: 0.8976\n","Epoch 6/10\n","625/625 [==============================] - 6s 9ms/step - loss: 0.1868 - accuracy: 0.9449 - val_loss: 0.3382 - val_accuracy: 0.8950\n","Epoch 7/10\n","625/625 [==============================] - 6s 9ms/step - loss: 0.1808 - accuracy: 0.9488 - val_loss: 0.3546 - val_accuracy: 0.8942\n","Epoch 8/10\n","625/625 [==============================] - 6s 9ms/step - loss: 0.1785 - accuracy: 0.9495 - val_loss: 0.3683 - val_accuracy: 0.8936\n","Epoch 9/10\n","625/625 [==============================] - 5s 9ms/step - loss: 0.1734 - accuracy: 0.9520 - val_loss: 0.3796 - val_accuracy: 0.8934\n","Epoch 10/10\n","625/625 [==============================] - 8s 14ms/step - loss: 0.1811 - accuracy: 0.9522 - val_loss: 0.3695 - val_accuracy: 0.8962\n","782/782 [==============================] - 17s 22ms/step - loss: 0.2701 - accuracy: 0.8960\n","Test acc: 0.896\n","Model: \"model_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 20000)]           0         \n","                                                                 \n"," dense_4 (Dense)             (None, 16)                320016    \n","                                                                 \n"," dropout_2 (Dropout)         (None, 16)                0         \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 17        \n","                                                                 \n","=================================================================\n","Total params: 320,033\n","Trainable params: 320,033\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","625/625 [==============================] - 17s 26ms/step - loss: 0.5684 - accuracy: 0.7199 - val_loss: 0.3361 - val_accuracy: 0.8778\n","Epoch 2/10\n","625/625 [==============================] - 6s 9ms/step - loss: 0.4084 - accuracy: 0.8324 - val_loss: 0.2994 - val_accuracy: 0.8818\n","Epoch 3/10\n","625/625 [==============================] - 6s 9ms/step - loss: 0.3476 - accuracy: 0.8601 - val_loss: 0.3208 - val_accuracy: 0.8778\n","Epoch 4/10\n","625/625 [==============================] - 6s 10ms/step - loss: 0.2953 - accuracy: 0.8809 - val_loss: 0.3499 - val_accuracy: 0.8708\n","Epoch 5/10\n","625/625 [==============================] - 6s 10ms/step - loss: 0.2774 - accuracy: 0.8885 - val_loss: 0.3423 - val_accuracy: 0.8676\n","Epoch 6/10\n","625/625 [==============================] - 6s 10ms/step - loss: 0.2623 - accuracy: 0.8949 - val_loss: 0.3602 - val_accuracy: 0.8574\n","Epoch 7/10\n","625/625 [==============================] - 6s 10ms/step - loss: 0.2572 - accuracy: 0.8971 - val_loss: 0.3711 - val_accuracy: 0.8668\n","Epoch 8/10\n","625/625 [==============================] - 6s 10ms/step - loss: 0.2505 - accuracy: 0.8964 - val_loss: 0.3705 - val_accuracy: 0.8588\n","Epoch 9/10\n","625/625 [==============================] - 6s 10ms/step - loss: 0.2465 - accuracy: 0.9019 - val_loss: 0.3900 - val_accuracy: 0.8638\n","Epoch 10/10\n","625/625 [==============================] - 6s 10ms/step - loss: 0.2396 - accuracy: 0.9003 - val_loss: 0.3775 - val_accuracy: 0.8552\n","782/782 [==============================] - 11s 13ms/step - loss: 0.3048 - accuracy: 0.8723\n","Test acc: 0.872\n","94.10 percent positive\n"]}],"source":["import string\n","\n","class Vectorizer:\n","    def standardize(self, text):\n","        text = text.lower()\n","        return \"\".join(char for char in text if char not in string.punctuation)\n","\n","    def tokenize(self, text):\n","        text = self.standardize(text)\n","        return text.split()\n","\n","    def make_vocabulary(self, dataset):\n","        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n","        for text in dataset:\n","            text = self.standardize(text)\n","            tokens = self.tokenize(text)\n","            for token in tokens:\n","                if token not in self.vocabulary:\n","                    self.vocabulary[token] = len(self.vocabulary)\n","        self.inverse_vocabulary = dict(\n","            (v, k) for k, v in self.vocabulary.items())\n","\n","    def encode(self, text):\n","        text = self.standardize(text)\n","        tokens = self.tokenize(text)\n","        return [self.vocabulary.get(token, 1) for token in tokens]\n","\n","    def decode(self, int_sequence):\n","        return \" \".join(\n","            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n","\n","vectorizer = Vectorizer()\n","dataset = [\n","    \"I write, erase, rewrite\",\n","    \"Erase again, and then\",\n","    \"A poppy blooms.\",\n","]\n","vectorizer.make_vocabulary(dataset)\n","\n","test_sentence = \"I write, rewrite, and still rewrite again\"\n","encoded_sentence = vectorizer.encode(test_sentence)\n","print(encoded_sentence)\n","\n","decoded_sentence = vectorizer.decode(encoded_sentence)\n","print(decoded_sentence)\n","\n","from tensorflow.keras.layers import TextVectorization\n","text_vectorization = TextVectorization(\n","    output_mode=\"int\",\n",")\n","\n","import re\n","import string\n","import tensorflow as tf\n","\n","def custom_standardization_fn(string_tensor):\n","    lowercase_string = tf.strings.lower(string_tensor)\n","    return tf.strings.regex_replace(\n","        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n","\n","def custom_split_fn(string_tensor):\n","    return tf.strings.split(string_tensor)\n","\n","text_vectorization = TextVectorization(\n","    output_mode=\"int\",\n","    standardize=custom_standardization_fn,\n","    split=custom_split_fn,\n",")\n","\n","\n","dataset = [\n","    \"I write, erase, rewrite\",\n","    \"Erase again, and then\",\n","    \"A poppy blooms.\",\n","]\n","text_vectorization.adapt(dataset)\n","\n","print(text_vectorization.get_vocabulary())\n","\n","vocabulary = text_vectorization.get_vocabulary()\n","test_sentence = \"I write, rewrite, and still rewrite again\"\n","encoded_sentence = text_vectorization(test_sentence)\n","print(encoded_sentence)\n","\n","\n","inverse_vocab = dict(enumerate(vocabulary))\n","decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n","print(decoded_sentence)\n","\n","!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -xf aclImdb_v1.tar.gz\n","\n","!rm -r aclImdb/train/unsup\n","!cat aclImdb/train/pos/4077_10.txt\n","\n","import os, pathlib, shutil, random\n","\n","base_dir = pathlib.Path(\"aclImdb\")\n","val_dir = base_dir / \"val\"\n","train_dir = base_dir / \"train\"\n","for category in (\"neg\", \"pos\"):\n","    os.makedirs(val_dir / category)\n","    files = os.listdir(train_dir / category)\n","    random.Random(1337).shuffle(files)\n","    num_val_samples = int(0.2 * len(files))\n","    val_files = files[-num_val_samples:]\n","    for fname in val_files:\n","        shutil.move(train_dir / category / fname,\n","                    val_dir / category / fname)\n","        \n","from tensorflow import keras\n","batch_size = 32\n","\n","train_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/train\", batch_size=batch_size\n",")\n","val_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/val\", batch_size=batch_size\n",")\n","test_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/test\", batch_size=batch_size\n",")\n","\n","for inputs, targets in train_ds:\n","    print(\"inputs.shape:\", inputs.shape)\n","    print(\"inputs.dtype:\", inputs.dtype)\n","    print(\"targets.shape:\", targets.shape)\n","    print(\"targets.dtype:\", targets.dtype)\n","    print(\"inputs[0]:\", inputs[0])\n","    print(\"targets[0]:\", targets[0])\n","    break\n","\n","text_vectorization = TextVectorization(\n","    max_tokens=20000,\n","    output_mode=\"multi_hot\",\n",")\n","text_only_train_ds = train_ds.map(lambda x, y: x)\n","text_vectorization.adapt(text_only_train_ds)\n","\n","binary_1gram_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","binary_1gram_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","binary_1gram_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","\n","for inputs, targets in binary_1gram_train_ds:\n","    print(\"inputs.shape:\", inputs.shape)\n","    print(\"inputs.dtype:\", inputs.dtype)\n","    print(\"targets.shape:\", targets.shape)\n","    print(\"targets.dtype:\", targets.dtype)\n","    print(\"inputs[0]:\", inputs[0])\n","    print(\"targets[0]:\", targets[0])\n","    break\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","def get_model(max_tokens=20000, hidden_dim=16):\n","    inputs = keras.Input(shape=(max_tokens,))\n","    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n","    x = layers.Dropout(0.5)(x)\n","    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","    model = keras.Model(inputs, outputs)\n","    model.compile(optimizer=\"rmsprop\",\n","                  loss=\"binary_crossentropy\",\n","                  metrics=[\"accuracy\"])\n","    return model\n","\n","model = get_model()\n","model.summary()\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n","                                    save_best_only=True)\n","]\n","model.fit(binary_1gram_train_ds.cache(),\n","          validation_data=binary_1gram_val_ds.cache(),\n","          epochs=10,\n","          callbacks=callbacks)\n","model = keras.models.load_model(\"binary_1gram.keras\")\n","print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")\n","\n","text_vectorization = TextVectorization(\n","    ngrams=2,\n","    max_tokens=20000,\n","    output_mode=\"multi_hot\",\n",")\n","\n","text_vectorization.adapt(text_only_train_ds)\n","binary_2gram_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","binary_2gram_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","binary_2gram_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","\n","model = get_model()\n","model.summary()\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n","                                    save_best_only=True)\n","]\n","model.fit(binary_2gram_train_ds.cache(),\n","          validation_data=binary_2gram_val_ds.cache(),\n","          epochs=10,\n","          callbacks=callbacks)\n","model = keras.models.load_model(\"binary_2gram.keras\")\n","print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")\n","\n","text_vectorization = TextVectorization(\n","    ngrams=2,\n","    max_tokens=20000,\n","    output_mode=\"count\"\n",")\n","\n","\n","text_vectorization = TextVectorization(\n","    ngrams=2,\n","    max_tokens=20000,\n","    output_mode=\"tf_idf\",\n",")\n","\n","text_vectorization.adapt(text_only_train_ds)\n","\n","tfidf_2gram_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","tfidf_2gram_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","tfidf_2gram_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","\n","model = get_model()\n","model.summary()\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n","                                    save_best_only=True)\n","]\n","model.fit(tfidf_2gram_train_ds.cache(),\n","          validation_data=tfidf_2gram_val_ds.cache(),\n","          epochs=10,\n","          callbacks=callbacks)\n","model = keras.models.load_model(\"tfidf_2gram.keras\")\n","print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")\n","\n","inputs = keras.Input(shape=(1,), dtype=\"string\")\n","processed_inputs = text_vectorization(inputs)\n","outputs = model(processed_inputs)\n","inference_model = keras.Model(inputs, outputs)\n","\n","import tensorflow as tf\n","raw_text_data = tf.convert_to_tensor([\n","    [\"That was an excellent movie, I loved it.\"],\n","])\n","predictions = inference_model(raw_text_data)\n","print(f\"{float(predictions[0] * 100):.2f} percent positive\")"]}]}