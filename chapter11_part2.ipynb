{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNdWDXOI6GQfD0V+M8OOadz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l_Pl9aYN0Z5y","executionInfo":{"status":"ok","timestamp":1672925326945,"user_tz":-210,"elapsed":4693465,"user":{"displayName":"Sayed Ali Sharifian","userId":"17165198639354888633"}},"outputId":"02c192b1-010d-486d-da6e-e4c6836fcf29"},"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 80.2M  100 80.2M    0     0  7240k      0  0:00:11  0:00:11 --:--:-- 15.0M\n","Found 20000 files belonging to 2 classes.\n","Found 5000 files belonging to 2 classes.\n","Found 25000 files belonging to 2 classes.\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 64)               5128448   \n"," l)                                                              \n","                                                                 \n"," dropout (Dropout)           (None, 64)                0         \n","                                                                 \n"," dense (Dense)               (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 5,128,513\n","Trainable params: 5,128,513\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","625/625 [==============================] - 175s 267ms/step - loss: 0.5265 - accuracy: 0.7461 - val_loss: 0.3562 - val_accuracy: 0.8618\n","Epoch 2/10\n","625/625 [==============================] - 170s 272ms/step - loss: 0.3323 - accuracy: 0.8770 - val_loss: 0.3597 - val_accuracy: 0.8522\n","Epoch 3/10\n","625/625 [==============================] - 170s 271ms/step - loss: 0.2748 - accuracy: 0.9037 - val_loss: 0.4262 - val_accuracy: 0.8530\n","Epoch 4/10\n","625/625 [==============================] - 170s 271ms/step - loss: 0.2242 - accuracy: 0.9226 - val_loss: 0.3161 - val_accuracy: 0.8790\n","Epoch 5/10\n","625/625 [==============================] - 170s 271ms/step - loss: 0.1961 - accuracy: 0.9330 - val_loss: 0.3070 - val_accuracy: 0.8742\n","Epoch 6/10\n","625/625 [==============================] - 170s 272ms/step - loss: 0.1667 - accuracy: 0.9441 - val_loss: 0.3407 - val_accuracy: 0.8584\n","Epoch 7/10\n","625/625 [==============================] - 170s 273ms/step - loss: 0.1482 - accuracy: 0.9524 - val_loss: 0.3916 - val_accuracy: 0.8754\n","Epoch 8/10\n","625/625 [==============================] - 170s 271ms/step - loss: 0.1283 - accuracy: 0.9589 - val_loss: 0.4005 - val_accuracy: 0.8696\n","Epoch 9/10\n","625/625 [==============================] - 170s 272ms/step - loss: 0.1069 - accuracy: 0.9660 - val_loss: 0.4584 - val_accuracy: 0.8382\n","Epoch 10/10\n","625/625 [==============================] - 169s 271ms/step - loss: 0.0887 - accuracy: 0.9730 - val_loss: 0.4121 - val_accuracy: 0.8698\n","782/782 [==============================] - 104s 132ms/step - loss: 0.3232 - accuracy: 0.8667\n","Test acc: 0.867\n","Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," embedding_1 (Embedding)     (None, None, 256)         5120000   \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 64)               73984     \n"," nal)                                                            \n","                                                                 \n"," dropout_1 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 5,194,049\n","Trainable params: 5,194,049\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","625/625 [==============================] - 48s 72ms/step - loss: 0.4605 - accuracy: 0.7876 - val_loss: 0.3602 - val_accuracy: 0.8490\n","Epoch 2/10\n","625/625 [==============================] - 46s 73ms/step - loss: 0.3014 - accuracy: 0.8902 - val_loss: 0.3674 - val_accuracy: 0.8520\n","Epoch 3/10\n","625/625 [==============================] - 44s 70ms/step - loss: 0.2354 - accuracy: 0.9165 - val_loss: 0.3231 - val_accuracy: 0.8860\n","Epoch 4/10\n","625/625 [==============================] - 46s 73ms/step - loss: 0.1935 - accuracy: 0.9315 - val_loss: 0.3224 - val_accuracy: 0.8786\n","Epoch 5/10\n","625/625 [==============================] - 46s 73ms/step - loss: 0.1633 - accuracy: 0.9441 - val_loss: 0.3311 - val_accuracy: 0.8738\n","Epoch 6/10\n","625/625 [==============================] - 45s 72ms/step - loss: 0.1318 - accuracy: 0.9563 - val_loss: 0.4203 - val_accuracy: 0.8434\n","Epoch 7/10\n","625/625 [==============================] - 44s 70ms/step - loss: 0.1199 - accuracy: 0.9608 - val_loss: 0.4457 - val_accuracy: 0.8800\n","Epoch 8/10\n","625/625 [==============================] - 44s 71ms/step - loss: 0.1012 - accuracy: 0.9679 - val_loss: 0.4045 - val_accuracy: 0.8774\n","Epoch 9/10\n","625/625 [==============================] - 44s 70ms/step - loss: 0.0811 - accuracy: 0.9727 - val_loss: 0.4090 - val_accuracy: 0.8712\n","Epoch 10/10\n","625/625 [==============================] - 45s 72ms/step - loss: 0.0687 - accuracy: 0.9788 - val_loss: 0.5150 - val_accuracy: 0.8726\n","782/782 [==============================] - 29s 35ms/step - loss: 0.3599 - accuracy: 0.8609\n","Test acc: 0.861\n","Model: \"model_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," embedding_2 (Embedding)     (None, None, 256)         5120000   \n","                                                                 \n"," bidirectional_2 (Bidirectio  (None, 64)               73984     \n"," nal)                                                            \n","                                                                 \n"," dropout_2 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 5,194,049\n","Trainable params: 5,194,049\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","625/625 [==============================] - 69s 95ms/step - loss: 0.3993 - accuracy: 0.8202 - val_loss: 0.3079 - val_accuracy: 0.8782\n","Epoch 2/10\n","625/625 [==============================] - 50s 79ms/step - loss: 0.2350 - accuracy: 0.9096 - val_loss: 0.2781 - val_accuracy: 0.8874\n","Epoch 3/10\n","625/625 [==============================] - 48s 77ms/step - loss: 0.1688 - accuracy: 0.9361 - val_loss: 0.2917 - val_accuracy: 0.8794\n","Epoch 4/10\n","625/625 [==============================] - 47s 75ms/step - loss: 0.1234 - accuracy: 0.9560 - val_loss: 0.3365 - val_accuracy: 0.8802\n","Epoch 5/10\n","625/625 [==============================] - 51s 81ms/step - loss: 0.0904 - accuracy: 0.9677 - val_loss: 0.3551 - val_accuracy: 0.8734\n","Epoch 6/10\n","625/625 [==============================] - 52s 83ms/step - loss: 0.0665 - accuracy: 0.9772 - val_loss: 0.3755 - val_accuracy: 0.8746\n","Epoch 7/10\n","625/625 [==============================] - 47s 74ms/step - loss: 0.0469 - accuracy: 0.9844 - val_loss: 0.4178 - val_accuracy: 0.8720\n","Epoch 8/10\n","625/625 [==============================] - 47s 75ms/step - loss: 0.0320 - accuracy: 0.9896 - val_loss: 0.4890 - val_accuracy: 0.8722\n","Epoch 9/10\n","625/625 [==============================] - 48s 77ms/step - loss: 0.0224 - accuracy: 0.9930 - val_loss: 0.5584 - val_accuracy: 0.8700\n","Epoch 10/10\n","625/625 [==============================] - 52s 83ms/step - loss: 0.0199 - accuracy: 0.9939 - val_loss: 0.5531 - val_accuracy: 0.8676\n","782/782 [==============================] - 36s 42ms/step - loss: 0.2908 - accuracy: 0.8811\n","Test acc: 0.881\n","--2023-01-05 13:12:11--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2023-01-05 13:12:11--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2023-01-05 13:12:12--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  5.05MB/s    in 2m 41s  \n","\n","2023-01-05 13:14:54 (5.10 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n","Found 400000 word vectors.\n","Model: \"model_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_4 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," embedding_3 (Embedding)     (None, None, 100)         2000000   \n","                                                                 \n"," bidirectional_3 (Bidirectio  (None, 64)               34048     \n"," nal)                                                            \n","                                                                 \n"," dropout_3 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 2,034,113\n","Trainable params: 34,113\n","Non-trainable params: 2,000,000\n","_________________________________________________________________\n","Epoch 1/10\n","625/625 [==============================] - 54s 72ms/step - loss: 0.5614 - accuracy: 0.7065 - val_loss: 0.5181 - val_accuracy: 0.7466\n","Epoch 2/10\n","625/625 [==============================] - 45s 71ms/step - loss: 0.4430 - accuracy: 0.7982 - val_loss: 0.4140 - val_accuracy: 0.8092\n","Epoch 3/10\n","625/625 [==============================] - 44s 70ms/step - loss: 0.3961 - accuracy: 0.8285 - val_loss: 0.3691 - val_accuracy: 0.8340\n","Epoch 4/10\n","625/625 [==============================] - 43s 69ms/step - loss: 0.3615 - accuracy: 0.8468 - val_loss: 0.3584 - val_accuracy: 0.8466\n","Epoch 5/10\n","625/625 [==============================] - 43s 68ms/step - loss: 0.3389 - accuracy: 0.8555 - val_loss: 0.3478 - val_accuracy: 0.8508\n","Epoch 6/10\n","625/625 [==============================] - 47s 75ms/step - loss: 0.3172 - accuracy: 0.8679 - val_loss: 0.3345 - val_accuracy: 0.8576\n","Epoch 7/10\n","625/625 [==============================] - 41s 66ms/step - loss: 0.2969 - accuracy: 0.8767 - val_loss: 0.3776 - val_accuracy: 0.8472\n","Epoch 8/10\n","625/625 [==============================] - 43s 68ms/step - loss: 0.2817 - accuracy: 0.8868 - val_loss: 0.3255 - val_accuracy: 0.8632\n","Epoch 9/10\n","625/625 [==============================] - 43s 69ms/step - loss: 0.2666 - accuracy: 0.8936 - val_loss: 0.3213 - val_accuracy: 0.8708\n","Epoch 10/10\n","625/625 [==============================] - 43s 69ms/step - loss: 0.2544 - accuracy: 0.8985 - val_loss: 0.3132 - val_accuracy: 0.8736\n","782/782 [==============================] - 28s 31ms/step - loss: 0.3044 - accuracy: 0.8732\n","Test acc: 0.873\n"]}],"source":["!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -xf aclImdb_v1.tar.gz\n","!rm -r aclImdb/train/unsup\n","\n","import os, pathlib, shutil, random\n","from tensorflow import keras\n","batch_size = 32\n","base_dir = pathlib.Path(\"aclImdb\")\n","val_dir = base_dir / \"val\"\n","train_dir = base_dir / \"train\"\n","for category in (\"neg\", \"pos\"):\n","    os.makedirs(val_dir / category)\n","    files = os.listdir(train_dir / category)\n","    random.Random(1337).shuffle(files)\n","    num_val_samples = int(0.2 * len(files))\n","    val_files = files[-num_val_samples:]\n","    for fname in val_files:\n","        shutil.move(train_dir / category / fname,\n","                    val_dir / category / fname)\n","\n","train_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/train\", batch_size=batch_size\n",")\n","val_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/val\", batch_size=batch_size\n",")\n","test_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/test\", batch_size=batch_size\n",")\n","text_only_train_ds = train_ds.map(lambda x, y: x)\n","\n","from tensorflow.keras import layers\n","\n","max_length = 600\n","max_tokens = 20000\n","text_vectorization = layers.TextVectorization(\n","    max_tokens=max_tokens,\n","    output_mode=\"int\",\n","    output_sequence_length=max_length,\n",")\n","text_vectorization.adapt(text_only_train_ds)\n","\n","int_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","int_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","int_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","\n","import tensorflow as tf\n","inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","embedded = tf.one_hot(inputs, depth=max_tokens)\n","x = layers.Bidirectional(layers.LSTM(32))(embedded)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.summary()\n","\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n","                                    save_best_only=True)\n","]\n","model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n","model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n","print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n","\n","embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)\n","\n","inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n","x = layers.Bidirectional(layers.LSTM(32))(embedded)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.summary()\n","\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n","                                    save_best_only=True)\n","]\n","model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n","model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n","print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n","\n","inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","embedded = layers.Embedding(\n","    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n","x = layers.Bidirectional(layers.LSTM(32))(embedded)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.summary()\n","\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n","                                    save_best_only=True)\n","]\n","model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n","model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n","print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n","\n","!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip -q glove.6B.zip\n","\n","import numpy as np\n","path_to_glove_file = \"glove.6B.100d.txt\"\n","\n","embeddings_index = {}\n","with open(path_to_glove_file) as f:\n","    for line in f:\n","        word, coefs = line.split(maxsplit=1)\n","        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n","        embeddings_index[word] = coefs\n","\n","print(f\"Found {len(embeddings_index)} word vectors.\")\n","\n","embedding_dim = 100\n","\n","vocabulary = text_vectorization.get_vocabulary()\n","word_index = dict(zip(vocabulary, range(len(vocabulary))))\n","\n","embedding_matrix = np.zeros((max_tokens, embedding_dim))\n","for word, i in word_index.items():\n","    if i < max_tokens:\n","        embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","embedding_layer = layers.Embedding(\n","    max_tokens,\n","    embedding_dim,\n","    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n","    trainable=False,\n","    mask_zero=True,\n",")\n","\n","inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","embedded = embedding_layer(inputs)\n","x = layers.Bidirectional(layers.LSTM(32))(embedded)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.summary()\n","\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n","                                    save_best_only=True)\n","]\n","model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n","model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n","print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"]}]}